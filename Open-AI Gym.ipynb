{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.10.9)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (1.2.0)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (2.21.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from gym) (1.15.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.0->gym) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.0->gym) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.0->gym) (2018.11.29)\n",
      "Requirement already satisfied: future in c:\\users\\dell\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow 1.12.0 has requirement keras-applications>=1.0.6, but you'll have keras-applications 1.0.2 which is incompatible.\n",
      "tensorflow 1.12.0 has requirement keras-preprocessing>=1.0.5, but you'll have keras-preprocessing 1.0.1 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this game action is discrete either 0 or 1\n",
    "env_name =  'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "\n",
    "print(env.action_space)\n",
    "\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()  # initial position\n",
    "\n",
    "for _ in range(200):\n",
    "    \n",
    "    \n",
    "#     action = env.action_space.sample()  # choose random action\n",
    "    pole_angle = state[2]\n",
    "    action = 0 if pole_angle < 0 else 1\n",
    "    state , reward , done , info  = env.step(action)\n",
    "    env.render()  # to show\n",
    "    \n",
    "env.close()\n",
    "\n",
    "# we can also use action_size = env.action_space.n\n",
    "# action = random.choice(range(action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_name =  'MountainCarContinuous-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(1,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    state , reward , done , info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (Q-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observtions are quantities to measure state \n",
    "# Q (s(t) , a(t)) = r(t+1) + gamma r(t+2) + (gamma)^2 r(t+3)---+---------\n",
    "# where gamma is discount factor\n",
    "# Q(s(t)  , a(t)) = r(t+1) + gamma . Q(s(t+1) , a(t+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q(st , at) = rt + 1 + G.Qmax(st + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random \n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNoSlip-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4','is_slippery' : False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "env_name = 'FrozenLakeNoSlip-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():   # agent is player , class is blueprint ie template\n",
    "    def __init__(self,env):  # self is this\n",
    "        self.is_discrete = type(env.action_space) == gym.spaces.discrete.Discrete\n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "        else:\n",
    "            self.action_low = self.action_space.low\n",
    "            self.action_high = self.action_space.high\n",
    "            self.action_space = env.action_space.shape\n",
    "            \n",
    "    def get_action(self,state):\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))\n",
    "            \n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low,\n",
    "                                      self.action_high,\n",
    "                                      self.action_shape)\n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "for ep in range(2):\n",
    "    \n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state , reward , done , info = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(Agent):\n",
    "    \n",
    "    \n",
    "    def __init__(self,env,discount_rate = 0.97 , learning_rate = 0.01):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n\n",
    "        self.exploration_rate = 1.0\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.q_table = 1e-4 * np.random.random([self.state_size,self.action_size])\n",
    "        \n",
    "        \n",
    "    def get_action(self,state):\n",
    "        q_state = self.q_table[state]\n",
    "        action_greedy = np.argmax(q_state)\n",
    "        action_random = super().get_action(state)\n",
    "        return action_random if random.random() < self.exploration_rate else action_greedy\n",
    "    \n",
    "    \n",
    "    def train(self,experience):\n",
    "        state , action , next_state , reward , done = experience\n",
    "        \n",
    "        q_next = self.q_table[next_state]\n",
    "        q_next = np.zeros([self.action_size]) if done else q_next\n",
    "        q_target = reward + self.discount_rate * np.max(q_next)\n",
    "        \n",
    "        q_update = q_target - self.q_table[state,action]  # gradient descend , mutlipy by learning rate\n",
    "        self.q_table[state , action] += self.learning_rate * q_update\n",
    "        \n",
    "        if done:\n",
    "            self.exploration_rate *= 0.99 # it reduces every_time\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:5 a:2\n",
      "Episode:99 Reward: 5.0 Explore: 0.36603234127322926\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "[[8.92725955e-05 6.92407727e-05 6.40747682e-05 7.30372916e-05]\n",
      " [1.73014496e-05 3.93695995e-05 4.35847721e-05 8.93770999e-05]\n",
      " [6.22673496e-05 1.34198721e-05 8.72495265e-05 1.16142250e-05]\n",
      " [9.52943169e-05 3.98775877e-05 1.42502668e-05 2.20608527e-05]\n",
      " [3.12350367e-05 3.97879046e-05 4.01602690e-05 2.49958230e-05]\n",
      " [4.75156285e-06 7.05590341e-05 7.47561780e-05 6.14505788e-05]\n",
      " [1.92675813e-05 8.94852162e-05 1.06352740e-05 9.16149307e-06]\n",
      " [1.55196901e-05 3.75067691e-05 6.45829488e-05 7.71718893e-05]\n",
      " [3.28185092e-05 7.74131902e-05 4.29415561e-06 4.73307514e-05]\n",
      " [6.15016135e-05 1.07935745e-05 7.09837774e-06 6.02893505e-05]\n",
      " [9.72480686e-05 1.62452296e-03 4.39235385e-05 8.14230864e-05]\n",
      " [6.91267787e-05 9.84427516e-05 1.79932281e-05 4.55205886e-05]\n",
      " [8.50467470e-05 8.29483742e-05 7.13601934e-05 8.63319819e-05]\n",
      " [8.80267515e-05 7.32638259e-05 7.61381231e-06 1.82176029e-05]\n",
      " [4.92873677e-05 1.01429935e-05 4.90634789e-02 4.14727879e-05]\n",
      " [1.14756294e-05 3.88572332e-05 1.48612660e-05 3.73178419e-05]]\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "\n",
    "for ep in range(100):\n",
    "    \n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state , reward , done , info = env.step(action)\n",
    "        agent.train((state , action , next_state , reward , done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        print(f\"s:{state} a:{action}\")\n",
    "        print(f\"Episode:{ep} Reward: {total_reward} Explore: {agent.exploration_rate}\")\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "        print(agent.q_table)\n",
    "        \n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning using nueral nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNAgent(Agent):\n",
    "    def __init__(self , env , discount_rate = 0.97 , learning_rate = 0.001):\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n\n",
    "        self.exploration_rate = 1.0\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.state_in = tf.placeholder(tf.int32 , shape = [1])   # for single row\n",
    "        self.action_in = tf.placeholder(tf.int32 , shape = [1])\n",
    "        self.target_in = tf.placeholder(tf.float32 , shape = [1])\n",
    "        \n",
    "        self.state = tf.one_hot(self.state_in , depth = self.state_size)\n",
    "        self.action = tf.one_hot(self.action_in , depth = self.action_size)\n",
    "        \n",
    "        self.q_state = tf.layers.dense(self.state,units = self.action_size , name = 'q_table')\n",
    "        self.q_action = tf.reduce_sum(tf.multiply(self.q_state , self.action) , axis = 1)  \n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.square(self.target_in - self.q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        \n",
    "    def get_action(self,state):\n",
    "        \n",
    "        q_state = self.sess.run(self.q_state,feed_dict = {self.state_in : [state]})\n",
    "        action_greedy = np.argmax(q_state)\n",
    "        action_random = super().get_action(state)\n",
    "        return action_random if random.random() < self.exploration_rate else action_greedy\n",
    "    \n",
    "    def train(self , experience):\n",
    "        state , action , next_state , reward , done = [[exp] for exp in  experience]\n",
    "        \n",
    "        q_next = self.sess.run(self.q_state , feed_dict = {self.state_in: next_state})\n",
    "        q_next[done] = np.zeros(self.action_size)\n",
    "        \n",
    "        q_target = reward + self.discount_rate * np.max(q_next)\n",
    "        feed = {self.state_in : state , self.action_in:action , self.target_in:q_target} \n",
    "        feed = self.sess.run(self.optimizer , feed_dict=feed)\n",
    "        \n",
    "        if done[0]: # in dict\n",
    "            self.exploration_rate *= 0.99\n",
    "            \n",
    "            \n",
    "    def __del__(self):\n",
    "        self.sess.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:5 a:2\n",
      "Episode:99 Reward: 3.0 Explore: 0.04904089407128576\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "[[ 0.09488706  1.2385808   0.73185223  0.9064963 ]\n",
      " [ 0.9520302   1.6178434   0.45488712 -0.2464204 ]\n",
      " [ 1.4635905   1.40746    -0.37101513 -1.2244873 ]\n",
      " [ 2.1063316   0.6461236  -0.00802113 -0.3023706 ]\n",
      " [-0.25106683 -0.4039877   0.98839456 -0.10738462]\n",
      " [-0.09967619 -0.22012347  0.30218738  0.05771732]\n",
      " [-1.2502596   0.8404912  -0.18106368  0.07739478]\n",
      " [-0.22580075 -0.19202867 -0.07839805  0.45591414]\n",
      " [-0.11133487 -1.9464959   1.2561946  -0.01497089]\n",
      " [ 1.1164619  -0.14226075  0.61659557 -0.55261505]\n",
      " [ 0.5494346   0.9701312  -2.1539297   0.2690145 ]\n",
      " [ 0.37470508  0.1430834   0.17389297 -0.24357486]\n",
      " [-0.4857506  -0.35783893  0.5065137   0.37363505]\n",
      " [-1.0324613  -0.3846952  -0.3108703   1.5437584 ]\n",
      " [ 0.14072025 -0.12420323 -0.42261964  0.5294349 ]\n",
      " [-0.4171778  -0.03310412  0.21447754  0.42376786]]\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "for ep in range(100):\n",
    "    \n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state , reward , done , info = env.step(action)\n",
    "        agent.train((state , action , next_state , reward , done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "\n",
    "        print(f\"s:{state} a:{action}\")\n",
    "        print(f\"Episode:{ep} Reward: {total_reward} Explore: {agent.exploration_rate}\")\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "        with tf.variable_scope('q_table' , reuse = True):\n",
    "            weights = agent.sess.run(tf.get_variable('kernel'))\n",
    "            print(weights)\n",
    "        \n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
